{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f9bf24-c1e4-492f-9035-826879345ad2",
   "metadata": {
    "id": "f5f9bf24-c1e4-492f-9035-826879345ad2"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Import Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50xdKNXJdlGS",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install symspellpy\n",
    "!pip install evaluate\n",
    "!pip install --upgrade transformers\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IHTYYqyGfOE4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fOo1Sp9MlWvl",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r7RPjuC0KNGR",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821263c4-6d7b-4077-896e-276b851ca34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import torch\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jChzIMH4oiYg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449957f-4302-45db-a226-9af70b101ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "from transformers import EarlyStoppingCallback #to stop training early when eval loss stops improving\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaTokenizerFast, RobertaConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support #metric functions\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neBAjLMidIwn",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cf4d9-6afc-49f3-8017-5ca8fbee7924",
   "metadata": {
    "id": "614cf4d9-6afc-49f3-8017-5ca8fbee7924"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Import Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gGmjvIYac_n7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aOMuLH1ke9t8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"/content/drive/MyDrive/Dataset-Hate-Speech-Detection.csv\" # main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61befe-d0a7-457d-91fe-ce94ea91e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path2, low_memory=False) # low memory for more more accurate type inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b4cb8-9ce9-4154-93b8-dd977761c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c46fc-5f6a-41a7-9cc9-5d05bf4eed76",
   "metadata": {
    "id": "ce7c46fc-5f6a-41a7-9cc9-5d05bf4eed76"
   },
   "source": [
    "Classification:\n",
    "\n",
    "- 0 - Hate Speech\n",
    "- 1 - Offensive Language\n",
    "- 2 - Neither/Neutral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95050622-c7b3-4198-97e5-36c9ff297578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VmCGq6bvphT8",
   "metadata": {
    "id": "VmCGq6bvphT8"
   },
   "source": [
    "NOTE: 24783 rows in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687ac9f-6fe5-4241-9765-7257a7040668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.index = df.index + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cc695-faa8-494f-8dc4-432a33338660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb469788-7726-4421-b0d3-6461c489f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2ed28-1355-4eb5-afba-6210f5d3377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e707e5-7246-4f87-9d35-eaee117bf4d2",
   "metadata": {
    "id": "e0e707e5-7246-4f87-9d35-eaee117bf4d2"
   },
   "source": [
    "NOTE: no null values & consistent datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf2a04-118a-4e3b-b529-99154aab6f71",
   "metadata": {
    "id": "1bcf2a04-118a-4e3b-b529-99154aab6f71"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Cleaning tweet column</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551278f-006d-43e6-baff-bda55b467589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e1f79-f5c3-4474-8928-38d8ebacd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.iloc[:10, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89510e-ac72-40ef-b47c-c1e56f3646a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefcacef-d10e-489a-8344-41393a69cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dea078-2c15-4c82-ae5c-971b8dd315ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = df['class'].value_counts()\n",
    "\n",
    "ax = class_count.plot(kind='bar')\n",
    "\n",
    "for i, value in enumerate(class_count):\n",
    "    ax.text(i, value + 0.5, str(value), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aytbJg9v8HNa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1430, 19190, 4163]\n",
    "labels = ['Hate speech', 'Offensive Language', 'Neutral Language']\n",
    "\n",
    "colors = ['#A9A9A9', '#B0C4DE', '#D2B48C']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(data, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "plt.title('Distribution by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40784d43-0db1-4df6-a439-aa24cc51470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.iloc[:5, :2]) # .iloc[row_indexer, column_indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b547d5-788f-43a5-9aff-a2bdebfde5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7166f-3f75-4059-9244-ceec29ed7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "dictionary_path = \"/content/drive/MyDrive/frequency_dictionary_en_82_765.txt\"  # path to the downloaded dictionary\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "#p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def correct_spelling_symspell(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        # Skip short words and stopwords to speed up\n",
    "        if len(word) < 3 or word in stop_words:\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (optional: keep the text if you want)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text).strip()\n",
    "    text = correct_spelling_symspell(text)\n",
    "    pattern = r'([{}])\\1+'.format(re.escape(punctuation))\n",
    "    text = re.sub(pattern, r'\\1', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['tweet_cleaned'] = df['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17603a-3f46-4ee3-8722-73d20b074b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_cleaned'] = df['tweet_cleaned'].str.replace(r'!\\s*rt\\s*:', '', regex=True).str.replace(r\"[^\\w\\s.,!?']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9hmaz2xq459C",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_cleaned'] = df['tweet_cleaned'].str.replace(r'\\brt\\b', '', regex=True).str.strip() # added line after checking more common words in 'tweet_cleaned' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e40bc-6ba6-4ba3-a4b1-21dae6c669ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Eid4OXcwQHL2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I53xYUSKAJUG",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='tweet_cleaned', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nf-krYZhrOIV",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fi_pR5tTrYdu",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('tweet', inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UETCQDE-rkdn",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='tweet_cleaned', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacZk9vpRUTN",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = df['class'].value_counts()\n",
    "\n",
    "ax = class_count.plot(kind='bar')\n",
    "\n",
    "for i, value in enumerate(class_count):\n",
    "    ax.text(i, value + 0.5, str(value), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lnpHd5zYroVc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nfv11N2sv_LG",
   "metadata": {
    "id": "Nfv11N2sv_LG"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Visualizations for insights with NLP techniques</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L5eLd4dDRdMf",
   "metadata": {
    "id": "L5eLd4dDRdMf"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Word cloud</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q1c8l-U4wHjj",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df['class'].unique():\n",
    "  test = ' '.join(df[df['class'] == c]['tweet_cleaned'])\n",
    "  wordcloud = WordCloud(width=800, height=800, background_color='white').generate(test)\n",
    "\n",
    "  plt.figure(figsize=(8, 8), facecolor=None)\n",
    "  plt.imshow(wordcloud)\n",
    "  plt.axis(\"off\")\n",
    "  plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l0MpAbnSpcT",
   "metadata": {
    "id": "8l0MpAbnSpcT"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Top Words per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SYl8JsWLxeZZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_top_words(texts, n=10):\n",
    "    words = \" \".join(texts).split()\n",
    "    words = [re.sub(r'[^\\w\\s]', '', w.lower()) for w in words]\n",
    "    words = [w for w in words if w and w not in stop_words]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "for c in df['class'].unique():\n",
    "    top_words = get_top_words(df[df['class'] == c]['tweet_cleaned'])\n",
    "    words, freqs = zip(*top_words)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(words, freqs, color='#A8D5BA')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Top Words in Class {c}\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EozhDUK0T0TH",
   "metadata": {
    "id": "EozhDUK0T0TH"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Sentiment Analysis -1 (negative) to +1 (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iN1m04sI3M2R",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ljIfoHgk23o6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer() # VADER sentiment analyzer from NLTK\n",
    "\n",
    "df['sentiment'] = df['tweet_cleaned'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "print(df[['class', 'sentiment']].groupby('class').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7IbnXaM-cJ3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment values (replace with your actual values if needed)\n",
    "sentiment_avg = df.groupby('class')['sentiment'].mean()\n",
    "\n",
    "colors = ['crimson' if val < 0 else 'seagreen' for val in sentiment_avg]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sentiment_avg.plot(kind='bar', color=colors)\n",
    "\n",
    "# Add sentiment values as text\n",
    "for i, value in enumerate(sentiment_avg):\n",
    "    va = 'bottom' if value >= 0 else 'top'\n",
    "    y_offset = 0.02 if value >= 0 else -0.02\n",
    "    ax.text(i, value + y_offset, f'{value:.2f}', ha='center', va=va, fontweight='bold')\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title('Average Sentiment per Class')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(sentiment_avg.min() - 0.1, sentiment_avg.max() + 0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429QVArMW-3P",
   "metadata": {
    "id": "429QVArMW-3P"
   },
   "source": [
    "##### TF-IDF / Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V9-SgAHL3bHy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "tfidf_matrix = tfidf.fit_transform(df['tweet_cleaned'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "df_tfidf = df_tfidf[(df_tfidf != 0).any(axis=1)]  # filter out rows with all zeros\n",
    "\n",
    "# Show most important words overall\n",
    "print(\"Top TF-IDF words across all tweets:\")\n",
    "print(df_tfidf.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v0fIUN8eJvjZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your list of top words\n",
    "top_words = ['bitch', 'bitches', 'like', 'hoes', 'pussy', 'hoe', 'nigga', 'ass', 'fuck', 'shit']\n",
    "\n",
    "tfidf = TfidfVectorizer(vocabulary=top_words)\n",
    "X = tfidf.fit_transform(df['tweet_cleaned'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "tfidf_df['class'] = df['class'].values\n",
    "\n",
    "avg_tfidf_per_class = tfidf_df.groupby('class').mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(avg_tfidf_per_class, annot=True, cmap='Reds', fmt='.2f')\n",
    "plt.title('Average TF-IDF of Top Words per Class')\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2i3jZYB-GMyX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "ax = sns.heatmap(avg_tfidf_per_class, annot=True, cmap='Reds', fmt='.2f', cbar=False, yticklabels=False)\n",
    "\n",
    "class_colors = ['red', 'orange', 'green']\n",
    "\n",
    "for y, color in enumerate(class_colors):\n",
    "    circle = Circle((-0.5, y + 0.5), 0.12, color=color, transform=ax.transData, clip_on=False)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.set_ylabel('')\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.title('Avg. TF-IDF of Top Words per Class', fontsize=12)\n",
    "plt.xlabel('Word', fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pWFMCLvm_hqj",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = ['bitch', 'bitches', 'like', 'hoes', 'pussy', 'hoe', 'nigga', 'ass', 'fuck', 'shit']\n",
    "\n",
    "rows = []\n",
    "\n",
    "for word in top_words:\n",
    "    for c in sorted(df['class'].unique()):\n",
    "        examples = df[(df['class'] == c) & (df['tweet_cleaned'].str.contains(fr'\\b{word}\\b', case=False, na=False))]\n",
    "        for tweet in examples['tweet_cleaned'].head(3):  # up to 3 examples\n",
    "            rows.append({'word': word, 'class': c, 'tweet': tweet})\n",
    "\n",
    "examples_df = pd.DataFrame(rows)\n",
    "\n",
    "examples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l_aHmNiIXoEX",
   "metadata": {
    "id": "l_aHmNiIXoEX"
   },
   "source": [
    "##### Bigrams - Pairs of consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OhuowUNHBl1Q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All classes\n",
    "classes = sorted(df['class'].unique())\n",
    "\n",
    "# Fit CountVectorizer on entire dataset to get vocabulary\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "X = vectorizer.fit_transform(df['tweet_cleaned'])\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Get top 10 bigrams overall\n",
    "bigram_freq = sorted(zip(bigrams, counts), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_bigrams, _ = zip(*bigram_freq)\n",
    "\n",
    "# Now get counts for these bigrams per class\n",
    "class_counts = []\n",
    "for c in classes:\n",
    "    # Vectorize only tweets from class c, using the SAME vocabulary to align features\n",
    "    Xc = CountVectorizer(ngram_range=(2, 2), stop_words='english', vocabulary=top_bigrams).fit_transform(df[df['class'] == c]['tweet_cleaned'])\n",
    "    class_counts.append(Xc.toarray().sum(axis=0))\n",
    "\n",
    "# Convert to numpy array for easier plotting: shape = (num_classes, num_bigrams)\n",
    "class_counts = np.array(class_counts)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bottom = np.zeros(len(top_bigrams))\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green']  # Adjust colors if you have more classes\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    ax.barh(top_bigrams, class_counts[i], left=bottom, color=colors[i], label=f'Class {c}')\n",
    "    bottom += class_counts[i]\n",
    "\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Top 10 Bigrams Frequency per Class (Stacked)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JHIGw-bTCRrM",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"bitch ass\"\n",
    "\n",
    "# Filter rows where 'tweet_cleaned' contains the exact phrase (case-insensitive)\n",
    "examples = df[df['tweet_cleaned'].str.contains(r'\\b' + phrase + r'\\b', case=False, na=False)]\n",
    "\n",
    "# Select relevant columns: class and tweet\n",
    "examples_df = examples[['class', 'tweet_cleaned']].rename(columns={'tweet_cleaned': 'tweet'})\n",
    "\n",
    "examples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTKFJd2xYehX",
   "metadata": {
    "id": "kTKFJd2xYehX"
   },
   "source": [
    "##### Topic Modeling using LDA - interactive pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ef5oywE70xA",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words='english')\n",
    "X = count_vec.fit_transform(df['tweet_cleaned'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    words = [count_vec.get_feature_names_out()[index] for index in topic.argsort()[-10:]]\n",
    "    print(f\"Topic #{i}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qH9Pv9-_mpnj",
   "metadata": {},
   "outputs": [],
   "source": [
    "_original_drop = pd.DataFrame.drop\n",
    "\n",
    "def patched_drop(self, labels=None, axis=0, *args, **kwargs):\n",
    "    return _original_drop(self, labels=labels, axis=axis, *args, **kwargs)\n",
    "\n",
    "pd.DataFrame.drop = patched_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcW1XO7RL82M",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dists = lda.transform(X)\n",
    "topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:, None]\n",
    "doc_lengths = X.sum(axis=1).A1\n",
    "vocab = count_vec.get_feature_names_out()\n",
    "term_frequency = np.array(X.sum(axis=0)).flatten()\n",
    "\n",
    "vis = pyLDAvis.prepare(\n",
    "    topic_term_dists,\n",
    "    doc_topic_dists,\n",
    "    doc_lengths,\n",
    "    vocab,\n",
    "    term_frequency\n",
    "#\n",
    "\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820BcB88pNQr",
   "metadata": {
    "id": "820BcB88pNQr"
   },
   "source": [
    "Note: the topics looks similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b1218-0c9c-4ce2-b52e-e83901a492a3",
   "metadata": {
    "id": "bc8b1218-0c9c-4ce2-b52e-e83901a492a3"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">Model building</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0521d8-c321-4b81-ab42-d3573b5deee5",
   "metadata": {
    "id": "ae0521d8-c321-4b81-ab42-d3573b5deee5"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">1 - Handling Imbalanced Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d981138-10c3-48b0-b2d6-5db8492a25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_tweets = df.groupby('tweet_cleaned')['class'].nunique()\n",
    "\n",
    "tweets_with_multiple_classes = multi_class_tweets[multi_class_tweets > 1]\n",
    "\n",
    "print(tweets_with_multiple_classes) # the same tweet with multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87c072-7b94-4a8d-8f5a-41be14aa05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['class'] == 0]   # All hate speech tweets\n",
    "df_1 = df[df['class'] == 1]   # All offensive tweets\n",
    "df_2 = df[df['class'] == 2]   # All neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba22e6-c236-4142-ab79-f3e6d0bb7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = df_1.shape[0]\n",
    "max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3LFRDjjpbRRt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555XuSobesn",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c17208-2824-44d9-b47d-120e0bd1725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3JtC-hModmFG",
   "metadata": {
    "id": "3JtC-hModmFG"
   },
   "source": [
    "Note: The model needs to be trained on additional data to predict subtle hate speech, various examples were checked at the evaluation stage, and the model couldn't spot them. New dataset to be added with subtle hatespeech examples (source huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J0-dIUwnjCJL",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = \"/content/drive/MyDrive/subtle_hp_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9LFzGPj_S0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(path3, low_memory=False) # low memory for more more accurate type inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mDOZ4yKikRmR",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s0Z3vPxDkRpl",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eBigADs1k3Wr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same cleanning process like in df\n",
    "\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "dictionary_path = \"/content/drive/MyDrive/frequency_dictionary_en_82_765.txt\"  # path to the downloaded dictionary\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def correct_spelling_symspell(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        if len(word) < 3 or word in stop_words:\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (optional: keep the text if you want)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text).strip()\n",
    "    text = correct_spelling_symspell(text)\n",
    "    pattern = r'([{}])\\1+'.format(re.escape(punctuation))\n",
    "    text = re.sub(pattern, r'\\1', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df2['tweet_cleaned'] = df2['cleaned_text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5IJ2aMZlTrB",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=['cleaned_text'])\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9od3fbPpkR7y",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5G1qUrhoh6g",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CItm-qjQEkAp",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='tweet_cleaned', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imhB9omioz3F",
   "metadata": {
    "id": "imhB9omioz3F"
   },
   "source": [
    "Note: Step completed to add new dataset to df, continuing with the balancing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kj0pnMzgpd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').nunique() # total number matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BuRol8JxpB1F",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting again per class\n",
    "\n",
    "df_0 = df[df['class'] == 0]   # All hate speech tweets\n",
    "df_1 = df[df['class'] == 1]   # All offensive tweets\n",
    "df_2 = df[df['class'] == 2]   # All neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24ff7e-d309-483e-acce-ca2770a10359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_upsampled = resample(df_0, replace=True, n_samples=max_size, random_state=42)\n",
    "df_2_upsampled = resample(df_2, replace=True, n_samples=max_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b8c4-6d1c-497c-bc0c-8635da8b5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df_0_upsampled, df_2_upsampled, df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01deb8-8f20-49ed-99cc-d2fdb0fe4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3aabe1-4487-4ffc-8fc5-64b30ce1e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140a214-a22b-4a19-87c0-3449195d739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb90550-8f34-44a2-af20-f959f9467d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc55b8-d194-4f3c-9cfa-8f85e7d68e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[\"char_length\"] = df_balanced[\"tweet_cleaned\"].apply(len)\n",
    "print(df_balanced[\"char_length\"].describe()) # to find the max length for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fd82c-8582-46cd-b5fe-21a4a68805b6",
   "metadata": {
    "id": "2e9fd82c-8582-46cd-b5fe-21a4a68805b6"
   },
   "source": [
    "##### <span style=\"color:#483D8B; font-weight:bold;\">2 - RoBERTa model bulding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e78743-2e91-41e3-bb4c-f17e36c704a1",
   "metadata": {
    "id": "44e78743-2e91-41e3-bb4c-f17e36c704a1"
   },
   "source": [
    "Using a lighter version or Roberta, distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee99492-6d6c-4979-b6e6-9ca7f25ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_balanced.groupby('class', group_keys=False).apply(lambda x: x.sample(frac=0.8, random_state=42)).reset_index(drop=True) #changed from 0.7 to 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26db3e-ecf4-41a2-b193-2c150235f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_reduced.drop(columns=['char_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7b38d-5af2-474e-8115-2337bb2eff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5167f-820d-4d1d-8a25-41832f5887e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced[\"tweet_cleaned\"]\n",
    "y = df_reduced[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b60dce-967b-48e1-8130-6885b5e2d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb08d1c-8412-4f50-ac08-6c4f04fcab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RobertaTokenizerFast.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318a4d3-2d14-45e5-9447-442f874ce3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=190) # changed the length (actuals length 147)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24721660-b26a-4bb0-a2d4-35a4ce23f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "test_dataset = TweetDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dDN2uf5CGuZc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\n",
    "        \"distilroberta-base\",\n",
    "        num_labels=3,\n",
    "        hidden_dropout_prob=0.3,                  # Increase dropout to help prevent overfitting\n",
    "        attention_probs_dropout_prob=0.3          # Dropout in attention layers too\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281abbde-3431-4fbd-be4b-38ba132e5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RobertaForSequenceClassification.from_pretrained(\"distilroberta-base\", config=config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100371df-2506-494b-8767-d93bd70acbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Where to save model outputs and checkpoints\n",
    "    num_train_epochs=8,     # Train up to 8 epochs - this is the best checkpoint (initially it was 10)\n",
    "    per_device_train_batch_size=20, # Number of samples per batch for training\n",
    "    per_device_eval_batch_size=20, # Number of samples per batch for evaluation\n",
    "    logging_dir='./logs',          # Directory for training logs\n",
    "    logging_steps=1000,             # Log training info every 1000 steps\n",
    "    # no_cuda=True  # to force CPU usage if needed\n",
    "    eval_strategy=\"epoch\",              # Run evaluation after every epoch to check model performance\n",
    "    save_strategy=\"epoch\",                    # Save model checkpoint after each epoch\n",
    "    load_best_model_at_end=True,              # After training, load the checkpoint with best evaluation loss!! (Check this again)\n",
    "    metric_for_best_model=\"eval_loss\",        # Use evaluation loss to determine which checkpoint is best\n",
    "    weight_decay=0.01                         # Add weight decay (L2 regularization) to prevent overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m2j1G8c4Hlnj",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2                 # Stop training if eval loss does not improve for 2 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_V_MODSIgMj",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d283374-6c71-4bf6-809c-30286cc32e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,          # function for evaluating metrics\n",
    "    callbacks=[early_stopping],               # Enable early stopping during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a342299-f44c-40e3-afd0-17f1fd0dff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jiI3nHliy2xW",
   "metadata": {
    "id": "jiI3nHliy2xW"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">3 - Classification report per class</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d6959-03da-4b88-94fa-2f6c6241da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "y_pred = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZPnFB17K8HRI",
   "metadata": {
    "id": "ZPnFB17K8HRI"
   },
   "source": [
    "First results: [452/452 00:17]\n",
    "{'eval_loss': 0.22755973041057587, 'eval_model_preparation_time': 0.0015, 'eval_runtime': 17.9547, 'eval_samples_per_second': 503.155, 'eval_steps_per_second': 25.174}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r2pEV-t7Yc5u",
   "metadata": {
    "id": "r2pEV-t7Yc5u"
   },
   "source": [
    " Second results: [442/442 00:21]\n",
    "{'eval_loss': 0.21381445229053497, 'eval_model_preparation_time': 0.0017, 'eval_runtime': 21.1788, 'eval_samples_per_second': 417.115, 'eval_steps_per_second': 20.87}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YJutcQeGe-GH",
   "metadata": {
    "id": "YJutcQeGe-GH"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">4 - Save model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wbpp47WemmH1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ET8YZbe1msSn",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/content/drive/MyDrive/best_model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-JLvtN06U4Dz",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/best_model'\n",
    "print(f\"Model and tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SF7-X4H9nF7i",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/best_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Lbota8TfTtk",
   "metadata": {
    "id": "6Lbota8TfTtk"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">5 - Predictions Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CxPOd3eTedJD",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03xdEI1ReqdY",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/MyDrive/best_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x-Phr3Xkb6tO",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-wrzkUYxfWBG",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mJoIr8s4busO",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRUG65dGcR9i",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ienRdlvxfxbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Run prediction\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get predicted class labels\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, preds))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='BuGn')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJzPaBfTeBB1",
   "metadata": {
    "id": "PJzPaBfTeBB1"
   },
   "source": [
    "CONCLUSIONS:\n",
    "\n",
    "Class 0 (F1 = 0.95): high precision and very high recall.\n",
    "\n",
    "Class 1 (F1 = 0.92): some true class 1 examples are being missed (recall = 0.89).\n",
    "\n",
    "Class 2 (F1 = 0.97): both precision and recall are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DE8ifvVqecWA",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('best_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Es7-4oKvffqD",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of misclassified samples:\", np.sum(preds != true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "STKQ2yEpjsxi",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "misclassified_indices = np.where(preds != true_labels)[0]\n",
    "\n",
    "print(\"Example misclassified tweet:\", X_test_reset.iloc[misclassified_indices[0]])\n",
    "print(\"True label:\", true_labels[misclassified_indices[0]])\n",
    "print(\"Predicted label:\", preds[misclassified_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wUPp-wR3kFaK",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "misclassified_counts = defaultdict(int)\n",
    "max_per_class = 10\n",
    "\n",
    "for idx in np.where(preds != true_labels)[0]:\n",
    "    true_class = true_labels[idx]\n",
    "    pred_class = preds[idx]\n",
    "\n",
    "    if misclassified_counts[true_class] < max_per_class:\n",
    "        tweet_text = X_test_reset.iloc[idx]\n",
    "        print(f\"\\nIndex {idx} | True: {true_class} | Pred: {pred_class}\")\n",
    "        print(\"Tweet:\", tweet_text)\n",
    "\n",
    "        misclassified_counts[true_class] += 1\n",
    "\n",
    "    if all(count >= max_per_class for count in misclassified_counts.values()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJiA_3l_kZ6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tweet_cleaned'].str.contains('julia roberts', case=False, na=False)] # predicted classification more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuwicwpfnR27",
   "metadata": {
    "id": "tuwicwpfnR27"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">6 - validate example subtle hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yEwFSoozIggJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hnk5Dy79ML2P",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"/content/drive/MyDrive/best_model\", tokenizer=\"/content/drive/MyDrive/best_model\") # Load the pipeline with your saved model\n",
    "\n",
    "texts = [\n",
    "    \"Some groups just dont deserve the same rights as others.\",\n",
    "    \"Oh sure, let's put another brilliant woman in charge, what could possibly go wrong.\",\n",
    "    \"Theyre always playing the victim card. You know the group I mean.\",\n",
    "    \"I love how Asians are always good at math. Wish I had that gene!\",\n",
    "    \"I identify as a toaster now. Respect my pronouns, lol.\",\n",
    "    \"Some cultures just dont value education like we do.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = classifier(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\" Class: {result['label']} | Confidence: {result['score']:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JnnODP7pplYT",
   "metadata": {
    "id": "JnnODP7pplYT"
   },
   "source": [
    "CONCLUSION:\n",
    "\n",
    "Model improved on identifying subtle hate speech but it needs further improvement as it misclassified some key example like \"Text: I identify as a toaster now. Respect my pronouns, lol.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fGEybctc79Q-",
   "metadata": {
    "id": "fGEybctc79Q-"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">Improt Data obtained via Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03yo3wbi8Bxg",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/reddit_comments.csv'\n",
    "\n",
    "df_rd = pd.read_csv(file_path)\n",
    "\n",
    "df_rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qSK0Q8Yi-k4Q",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6_oDLNUH_WIS",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-e6Md3heNFQ8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XVWT7BqAMAtc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same cleanning process like in df\n",
    "\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "dictionary_path = \"/content/drive/MyDrive/frequency_dictionary_en_82_765.txt\"  # path to the downloaded dictionary\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def correct_spelling_symspell(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        if len(word) < 3 or word in stop_words:\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (optional: keep the text if you want)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text).strip()\n",
    "    text = correct_spelling_symspell(text)\n",
    "    pattern = r'([{}])\\1+'.format(re.escape(punctuation))\n",
    "    text = re.sub(pattern, r'\\1', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df_rd['cleaned_comment'] = df_rd['comment'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wMBj2zgHI09-",
   "metadata": {
    "id": "wMBj2zgHI09-"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">Predict class on Redit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AfSX81dXOQh9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd_2 = df_rd.drop(columns=['comment'])\n",
    "df_rd_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kXTnu865_REH",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/best_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/best_model')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqhzcbWn_k9-",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_rd['comment'].tolist()\n",
    "\n",
    "inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\") # Neural networks process fixed-length inputs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1d6hpSD_uYR",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 20  # smaller batches use less memory\n",
    "\n",
    "for i in range(0, len(df_rd_2), batch_size):\n",
    "    batch = df_rd_2['cleaned_comment'][i:i+batch_size].tolist()\n",
    "    inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    predictions.extend(preds.tolist())\n",
    "\n",
    "df_rd_2['predicted_class'] = predictions\n",
    "df_rd_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6Jxhc2BIWUx",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd_2['predicted_class'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kQdutWUlO5OF",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_rd.drop(columns=['cleaned_comment']).join(df_rd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FCKEmfPYPRJ4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yf5fnD4dJFEP",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_combined[df_combined['predicted_class'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50uNCLB6RG7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df_combined.groupby('predicted_class').apply(lambda x: x.sample(30, random_state=42)).reset_index(drop=True)\n",
    "sampled_df.to_csv('/content/drive/MyDrive/sampled_comments_per_class.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NpGwYM3rQDZ6",
   "metadata": {
    "id": "NpGwYM3rQDZ6"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">Create Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KpebTUI0MfmY",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers safetensors streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RAt9tGjYaWw0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wq3vU36GVJxM",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"/content/drive/MyDrive/best_model\"\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "label_map = {0: \"hate speech\", 1: \"offensive language\", 2: \"neutral language\"}\n",
    "\n",
    "st.title(\"Hate speech detector\")\n",
    "\n",
    "text = st.text_input(\"Enter text to classify:\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    if not text.strip():\n",
    "        st.warning(\"Please enter some text!\")\n",
    "    else:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        pred_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "        st.write(\"Prediction:\", label_map.get(pred_idx, \"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bN5Fr9duRGvC",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit pyngrok transformers\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Set your ngrok authtoken\n",
    "NGROK_AUTH_TOKEN = \"2zNQYRoaf4qEKJ2xl3D6iPlUxoy_sFhQusi2Qyio9acW1J7D\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start Streamlit in the background\n",
    "proc = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
    "\n",
    "# Open a tunnel on port 8501\n",
    "public_url = ngrok.connect(8501).public_url\n",
    "\n",
    "print(f\" Your Streamlit app is live at: {public_url}\")\n",
    "\n",
    "# Keep the cell alive to maintain the tunnel\n",
    "while True:\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pki6L_Jy0N9B",
   "metadata": {
    "id": "Pki6L_Jy0N9B"
   },
   "source": [
    "#### <span style=\"color:#483D8B; font-weight:bold;\">Create Streamlit App on local machine to be more stable for presenting - code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QX4KeCLG1ojE",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r best_model_final.zip /content/drive/MyDrive/best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BXuN-RbT2Hof",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/best_model_final.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dy9ZB-OkD6ei",
   "metadata": {
    "id": "dy9ZB-OkD6ei"
   },
   "source": [
    "**Code in .py file:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117eycisD9Tn",
   "metadata": {
    "id": "117eycisD9Tn"
   },
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(r\"C:\\Users\\User\\Desktop\\Ironhack_DA\\Final-project-hate-speech-detection-with-NLP\\best_model_final\")\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_model():\n",
    "    model_path_posix = MODEL_PATH.as_posix()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_posix, local_files_only=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path_posix, local_files_only=True)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_class_id = torch.argmax(probs).item()\n",
    "    confidence = probs[0, predicted_class_id].item()\n",
    "    return predicted_class_id, confidence\n",
    "\n",
    "def main():\n",
    "    st.title(\"Hate Speech Detector \")\n",
    "\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    user_input = st.text_area(\"Enter text to classify\", height=150)\n",
    "\n",
    "    if st.button(\"Predict\"):\n",
    "        if user_input.strip() == \"\":\n",
    "            st.warning(\"Please enter some text to classify.\")\n",
    "        else:\n",
    "            pred_class, conf = predict(user_input, tokenizer, model)\n",
    "            st.write(f\"**Predicted class:** {pred_class}\")\n",
    "            st.write(f\"**Confidence:** {conf:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeCJpjxJXkD",
   "metadata": {
    "id": "0aeCJpjxJXkD"
   },
   "source": [
    "Link to Streamlit App: http://localhost:8501/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6qAL7KHULhi",
   "metadata": {
    "id": "d6qAL7KHULhi"
   },
   "source": [
    "## Via Hugging face cloud (Sreamlit_App.py):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZouaBFPGUJXR",
   "metadata": {
    "id": "ZouaBFPGUJXR"
   },
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_REPO = \"GeorgiosKoutroumanos/NLP-Roberta-HP-detection\"\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_model():\n",
    "    st.write(\"Loading model from Hugging Face Hub...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_REPO)\n",
    "    model.eval()\n",
    "    st.write(\"Model loaded successfully!\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_class_id = torch.argmax(probs).item()\n",
    "    confidence = probs[0, predicted_class_id].item()\n",
    "    return predicted_class_id, confidence\n",
    "\n",
    "def main():\n",
    "    st.title(\"Hate Speech Detector \")\n",
    "\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    user_input = st.text_area(\"Enter text to classify\", height=150)\n",
    "\n",
    "    if st.button(\"Predict\"):\n",
    "        if user_input.strip() == \"\":\n",
    "            st.warning(\"Please enter some text to classify.\")\n",
    "        else:\n",
    "            pred_class, conf = predict(user_input, tokenizer, model)\n",
    "            st.write(f\"**Predicted class:** {pred_class}\")\n",
    "            st.write(f\"**Confidence:** {conf:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wiOpESWnUjMy",
   "metadata": {
    "id": "wiOpESWnUjMy"
   },
   "source": [
    "https://final-project-hate-speech-detection-with-nlp.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184412d8-f706-404e-bd56-1ac4ccb4ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Full path to your notebook file\n",
    "notebook_path = r\"C:\\Users\\User\\Desktop\\Ironhack_DA\\Final-project-hate-speech-detection-with-NLP\\MAIN.ipynb\"\n",
    "\n",
    "# Load the notebook\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove widgets metadata if it exists\n",
    "if \"widgets\" in nb[\"metadata\"]:\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "# Save the cleaned notebook\n",
    "with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook cleaned and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb04bb-d05b-4929-ae32-22021c8d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Full path to your notebook file\n",
    "notebook_path = r\"C:\\Users\\User\\Desktop\\Ironhack_DA\\Final-project-hate-speech-detection-with-NLP\\MAIN.ipynb\"\n",
    "\n",
    "# Load notebook\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check and delete corrupted widget metadata\n",
    "widgets = nb.get(\"metadata\", {}).get(\"widgets\", {})\n",
    "if \"application/vnd.jupyter.widget-state+json\" in widgets:\n",
    "    print(\"Found widget metadata. Removing it...\")\n",
    "    del widgets[\"application/vnd.jupyter.widget-state+json\"]\n",
    "    # Clean up empty 'widgets' dict if necessary\n",
    "    if not widgets:\n",
    "        del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "# Save the cleaned notebook\n",
    "with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0f355-6949-449d-b051-e2351bb50e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
