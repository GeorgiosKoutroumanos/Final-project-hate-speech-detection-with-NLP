{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5f9bf24-c1e4-492f-9035-826879345ad2",
      "metadata": {
        "id": "f5f9bf24-c1e4-492f-9035-826879345ad2"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">Import Libraries</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install symspellpy\n",
        "!pip install evaluate\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50xdKNXJdlGS",
        "outputId": "0a41c74f-0849-4320-9992-94abed5deffe"
      },
      "id": "50xdKNXJdlGS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.6 symspellpy-6.9.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.4\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7RPjuC0KNGR",
        "outputId": "3670766b-f877-4ef1-b0cf-23f05b615a5b"
      },
      "id": "r7RPjuC0KNGR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.52.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall preprocessing"
      ],
      "metadata": {
        "id": "mCPw0bO_lxJ3"
      },
      "id": "mCPw0bO_lxJ3",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e4a6660-b2d4-4296-b06a-0c4dde31cfad",
      "metadata": {
        "id": "7e4a6660-b2d4-4296-b06a-0c4dde31cfad"
      },
      "outputs": [],
      "source": [
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "821263c4-6d7b-4077-896e-276b851ca34c",
      "metadata": {
        "id": "821263c4-6d7b-4077-896e-276b851ca34c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "#import preprocessor as p\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0449957f-4302-45db-a226-9af70b101ff5",
      "metadata": {
        "id": "0449957f-4302-45db-a226-9af70b101ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "bbd1a669-6c1a-4505-c7f3-375b0aa0e689"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1733936707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStoppingCallback\u001b[0m \u001b[0;31m#to stop training early when eval loss stops improving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Integrations must be imported before ML frameworks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m from ..utils import (\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mverify_tp_plan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m from .pytorch_utils import (  # noqa: F401\n\u001b[1;32m     75\u001b[0m     \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_d_fine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFineForObjectDetectionLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_d_fine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .loss_for_object_detection import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbox_iou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_KerasLazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mself_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# incompatibilities before we trigger them (which would typically result in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# SIGILL).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_cpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0m_pywrap_cpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInfoAboutUnusedCPUFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import evaluate\n",
        "from transformers import EarlyStoppingCallback #to stop training early when eval loss stops improving\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaTokenizerFast, RobertaConfig\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support #metric functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614cf4d9-6afc-49f3-8017-5ca8fbee7924",
      "metadata": {
        "id": "614cf4d9-6afc-49f3-8017-5ca8fbee7924"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">Import Dataset</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gGmjvIYac_n7"
      },
      "id": "gGmjvIYac_n7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6156e0a-dc3c-4765-b995-bc5115bb5fd6",
      "metadata": {
        "id": "a6156e0a-dc3c-4765-b995-bc5115bb5fd6"
      },
      "outputs": [],
      "source": [
        "path = r\"C:\\Users\\User\\Desktop\\Ironhack_DA\\Final-project-hate-speech-detection-with-NLP\\Dataset-Hate-Speech-Detection.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path2 = \"/content/drive/MyDrive/Dataset-Hate-Speech-Detection.csv\""
      ],
      "metadata": {
        "id": "aOMuLH1ke9t8"
      },
      "id": "aOMuLH1ke9t8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c61befe-d0a7-457d-91fe-ce94ea91e693",
      "metadata": {
        "id": "3c61befe-d0a7-457d-91fe-ce94ea91e693"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path2, low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36b4cb8-9ce9-4154-93b8-dd977761c28a",
      "metadata": {
        "id": "c36b4cb8-9ce9-4154-93b8-dd977761c28a"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7c46fc-5f6a-41a7-9cc9-5d05bf4eed76",
      "metadata": {
        "id": "ce7c46fc-5f6a-41a7-9cc9-5d05bf4eed76"
      },
      "source": [
        "Classification:\n",
        "\n",
        "- 0 - Hate Speech\n",
        "- 1 - Offensive Language\n",
        "- 2 - Neither/Neutral content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95050622-c7b3-4198-97e5-36c9ff297578",
      "metadata": {
        "id": "95050622-c7b3-4198-97e5-36c9ff297578"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4687ac9f-6fe5-4241-9765-7257a7040668",
      "metadata": {
        "id": "4687ac9f-6fe5-4241-9765-7257a7040668"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True, inplace=True)\n",
        "df.index = df.index + 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59cc695-faa8-494f-8dc4-432a33338660",
      "metadata": {
        "id": "a59cc695-faa8-494f-8dc4-432a33338660"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb469788-7726-4421-b0d3-6461c489f231",
      "metadata": {
        "id": "eb469788-7726-4421-b0d3-6461c489f231"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c2ed28-1355-4eb5-afba-6210f5d3377d",
      "metadata": {
        "id": "57c2ed28-1355-4eb5-afba-6210f5d3377d"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e707e5-7246-4f87-9d35-eaee117bf4d2",
      "metadata": {
        "id": "e0e707e5-7246-4f87-9d35-eaee117bf4d2"
      },
      "source": [
        "NOTE: no null values & consistent datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcf2a04-118a-4e3b-b529-99154aab6f71",
      "metadata": {
        "id": "1bcf2a04-118a-4e3b-b529-99154aab6f71"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">Cleaning tweet column</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8551278f-006d-43e6-baff-bda55b467589",
      "metadata": {
        "id": "8551278f-006d-43e6-baff-bda55b467589"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3e1f79-f5c3-4474-8928-38d8ebacd761",
      "metadata": {
        "id": "1a3e1f79-f5c3-4474-8928-38d8ebacd761"
      },
      "outputs": [],
      "source": [
        "display(df.iloc[:10, :2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c89510e-ac72-40ef-b47c-c1e56f3646a6",
      "metadata": {
        "id": "0c89510e-ac72-40ef-b47c-c1e56f3646a6"
      },
      "outputs": [],
      "source": [
        "df[df['class'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefcacef-d10e-489a-8344-41393a69cfa5",
      "metadata": {
        "id": "aefcacef-d10e-489a-8344-41393a69cfa5"
      },
      "outputs": [],
      "source": [
        "df.groupby('class').nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0dea078-2c15-4c82-ae5c-971b8dd315ea",
      "metadata": {
        "id": "b0dea078-2c15-4c82-ae5c-971b8dd315ea"
      },
      "outputs": [],
      "source": [
        "class_count = df['class'].value_counts()\n",
        "\n",
        "ax = class_count.plot(kind='bar')\n",
        "\n",
        "for i, value in enumerate(class_count):\n",
        "    ax.text(i, value + 0.5, str(value), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40784d43-0db1-4df6-a439-aa24cc51470d",
      "metadata": {
        "id": "40784d43-0db1-4df6-a439-aa24cc51470d"
      },
      "outputs": [],
      "source": [
        "display(df.iloc[:5, :2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b547d5-788f-43a5-9aff-a2bdebfde5fe",
      "metadata": {
        "id": "81b547d5-788f-43a5-9aff-a2bdebfde5fe"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a7166f-3f75-4059-9244-ceec29ed7515",
      "metadata": {
        "id": "e4a7166f-3f75-4059-9244-ceec29ed7515"
      },
      "outputs": [],
      "source": [
        "max_edit_distance_dictionary = 2\n",
        "prefix_length = 7\n",
        "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
        "\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"  # path to the downloaded dictionary\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "#p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = string.punctuation\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "def correct_spelling_symspell(text):\n",
        "    corrected_words = []\n",
        "    for word in text.split():\n",
        "        # Skip short words and stopwords to speed up\n",
        "        if len(word) < 3 or word in stop_words:\n",
        "            corrected_words.append(word)\n",
        "            continue\n",
        "\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "        if suggestions:\n",
        "            corrected_words.append(suggestions[0].term)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def clean_tweet(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags (optional: keep the text if you want)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text).strip()\n",
        "    text = correct_spelling_symspell(text)\n",
        "    pattern = r'([{}])\\1+'.format(re.escape(punctuation))\n",
        "    text = re.sub(pattern, r'\\1', text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['tweet_cleaned'] = df['tweet'].apply(clean_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e17603a-3f46-4ee3-8722-73d20b074b80",
      "metadata": {
        "id": "4e17603a-3f46-4ee3-8722-73d20b074b80"
      },
      "outputs": [],
      "source": [
        "df['tweet_cleaned'] = df['tweet_cleaned'].str.replace(r'!\\s*rt\\s*:', '', regex=True).str.replace(r\"[^\\w\\s.,!?']\", \"\", regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4e40bc-6ba6-4ba3-a4b1-21dae6c669ea",
      "metadata": {
        "scrolled": true,
        "id": "1b4e40bc-6ba6-4ba3-a4b1-21dae6c669ea"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word cloud, to find common words in each class\n",
        "- NLP techniques to get insights"
      ],
      "metadata": {
        "id": "R2JJdXpaoMz7"
      },
      "id": "R2JJdXpaoMz7"
    },
    {
      "cell_type": "markdown",
      "id": "bc8b1218-0c9c-4ce2-b52e-e83901a492a3",
      "metadata": {
        "id": "bc8b1218-0c9c-4ce2-b52e-e83901a492a3"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">Model bulding</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0521d8-c321-4b81-ab42-d3573b5deee5",
      "metadata": {
        "id": "ae0521d8-c321-4b81-ab42-d3573b5deee5"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">1 - Handling Imbalanced Data</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72112fb5-8d17-4210-904d-5d92ea98a9a0",
      "metadata": {
        "id": "72112fb5-8d17-4210-904d-5d92ea98a9a0"
      },
      "outputs": [],
      "source": [
        "df.groupby('class').nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45e1e34-a8e4-47bf-a33f-801a5f366bcf",
      "metadata": {
        "id": "f45e1e34-a8e4-47bf-a33f-801a5f366bcf"
      },
      "outputs": [],
      "source": [
        "df.drop('tweet', inplace = True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa79405-7352-46e0-ac0e-beb5cdce905c",
      "metadata": {
        "id": "0aa79405-7352-46e0-ac0e-beb5cdce905c"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset='tweet_cleaned', keep='first').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d981138-10c3-48b0-b2d6-5db8492a25fa",
      "metadata": {
        "id": "6d981138-10c3-48b0-b2d6-5db8492a25fa"
      },
      "outputs": [],
      "source": [
        "multi_class_tweets = df.groupby('tweet_cleaned')['class'].nunique()\n",
        "\n",
        "tweets_with_multiple_classes = multi_class_tweets[multi_class_tweets > 1]\n",
        "\n",
        "print(tweets_with_multiple_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c87c072-7b94-4a8d-8f5a-41be14aa05db",
      "metadata": {
        "id": "4c87c072-7b94-4a8d-8f5a-41be14aa05db"
      },
      "outputs": [],
      "source": [
        "df_0 = df[df['class'] == 0]   # All hate speech tweets\n",
        "df_1 = df[df['class'] == 1]   # All offensive tweets\n",
        "df_2 = df[df['class'] == 2]   # All neutral tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aba22e6-c236-4142-ab79-f3e6d0bb7535",
      "metadata": {
        "id": "2aba22e6-c236-4142-ab79-f3e6d0bb7535"
      },
      "outputs": [],
      "source": [
        "max_size = df_1.shape[0]\n",
        "max_size\n",
        "\n",
        "#there are duplicate rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c17208-2824-44d9-b47d-120e0bd1725e",
      "metadata": {
        "id": "11c17208-2824-44d9-b47d-120e0bd1725e"
      },
      "outputs": [],
      "source": [
        "df.groupby('class').nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae24ff7e-d309-483e-acce-ca2770a10359",
      "metadata": {
        "id": "ae24ff7e-d309-483e-acce-ca2770a10359"
      },
      "outputs": [],
      "source": [
        "df_0_upsampled = resample(df_0, replace=True, n_samples=max_size, random_state=42)\n",
        "df_2_upsampled = resample(df_2, replace=True, n_samples=max_size, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d0b8c4-6d1c-497c-bc0c-8635da8b5a54",
      "metadata": {
        "id": "a3d0b8c4-6d1c-497c-bc0c-8635da8b5a54"
      },
      "outputs": [],
      "source": [
        "df_balanced = pd.concat([df_0_upsampled, df_2_upsampled, df_1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee01deb8-8f20-49ed-99cc-d2fdb0fe4292",
      "metadata": {
        "id": "ee01deb8-8f20-49ed-99cc-d2fdb0fe4292"
      },
      "outputs": [],
      "source": [
        "df_balanced.groupby('class').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3aabe1-4487-4ffc-8fc5-64b30ce1e8b7",
      "metadata": {
        "id": "6b3aabe1-4487-4ffc-8fc5-64b30ce1e8b7"
      },
      "outputs": [],
      "source": [
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0140a214-a22b-4a19-87c0-3449195d739f",
      "metadata": {
        "id": "0140a214-a22b-4a19-87c0-3449195d739f"
      },
      "outputs": [],
      "source": [
        "df_balanced.groupby('class').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb90550-8f34-44a2-af20-f959f9467d42",
      "metadata": {
        "id": "cfb90550-8f34-44a2-af20-f959f9467d42"
      },
      "outputs": [],
      "source": [
        "df_balanced.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88fc55b8-d194-4f3c-9cfa-8f85e7d68e0a",
      "metadata": {
        "id": "88fc55b8-d194-4f3c-9cfa-8f85e7d68e0a"
      },
      "outputs": [],
      "source": [
        "df_balanced[\"char_length\"] = df_balanced[\"tweet_cleaned\"].apply(len)\n",
        "print(df_balanced[\"char_length\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e9fd82c-8582-46cd-b5fe-21a4a68805b6",
      "metadata": {
        "id": "2e9fd82c-8582-46cd-b5fe-21a4a68805b6"
      },
      "source": [
        "##### <span style=\"color:#483D8B; font-weight:bold;\">2 - RoBERTa model bulding</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e78743-2e91-41e3-bb4c-f17e36c704a1",
      "metadata": {
        "id": "44e78743-2e91-41e3-bb4c-f17e36c704a1"
      },
      "source": [
        "Using a lighter version or Roberta, distilroberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee99492-6d6c-4979-b6e6-9ca7f25ba0f7",
      "metadata": {
        "id": "6ee99492-6d6c-4979-b6e6-9ca7f25ba0f7"
      },
      "outputs": [],
      "source": [
        "df_reduced = df_balanced.groupby('class', group_keys=False).apply(lambda x: x.sample(frac=0.8, random_state=42)).reset_index(drop=True) #changed from 0.7 to 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b26db3e-ecf4-41a2-b193-2c150235f796",
      "metadata": {
        "id": "4b26db3e-ecf4-41a2-b193-2c150235f796"
      },
      "outputs": [],
      "source": [
        "df_reduced = df_reduced.drop(columns=['char_length'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa7b38d-5af2-474e-8115-2337bb2eff32",
      "metadata": {
        "id": "9aa7b38d-5af2-474e-8115-2337bb2eff32"
      },
      "outputs": [],
      "source": [
        "df_reduced.groupby('class').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec5167f-820d-4d1d-8a25-41832f5887e8",
      "metadata": {
        "id": "dec5167f-820d-4d1d-8a25-41832f5887e8"
      },
      "outputs": [],
      "source": [
        "X = df_reduced[\"tweet_cleaned\"]\n",
        "y = df_reduced[\"class\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b60dce-967b-48e1-8130-6885b5e2d37e",
      "metadata": {
        "id": "91b60dce-967b-48e1-8130-6885b5e2d37e"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb08d1c-8412-4f50-ac08-6c4f04fcab6e",
      "metadata": {
        "id": "5cb08d1c-8412-4f50-ac08-6c4f04fcab6e"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"distilroberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0318a4d3-2d14-45e5-9447-442f874ce3df",
      "metadata": {
        "id": "0318a4d3-2d14-45e5-9447-442f874ce3df"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=150) # changed the length (actuals length 147)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24721660-b26a-4bb0-a2d4-35a4ce23f9b5",
      "metadata": {
        "id": "24721660-b26a-4bb0-a2d4-35a4ce23f9b5"
      },
      "outputs": [],
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TweetDataset(train_encodings, y_train)\n",
        "test_dataset = TweetDataset(test_encodings, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = RobertaConfig.from_pretrained(\n",
        "        \"distilroberta-base\",\n",
        "        num_labels=3,\n",
        "        hidden_dropout_prob=0.3,                  # Increase dropout to help prevent overfitting\n",
        "        attention_probs_dropout_prob=0.3          # Dropout in attention layers too\n",
        ")"
      ],
      "metadata": {
        "id": "dDN2uf5CGuZc"
      },
      "id": "dDN2uf5CGuZc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281abbde-3431-4fbd-be4b-38ba132e5663",
      "metadata": {
        "id": "281abbde-3431-4fbd-be4b-38ba132e5663"
      },
      "outputs": [],
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"distilroberta-base\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100371df-2506-494b-8767-d93bd70acbd4",
      "metadata": {
        "id": "100371df-2506-494b-8767-d93bd70acbd4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Where to save model outputs and checkpoints\n",
        "    num_train_epochs=10,     # Train up to 10 epochs (early stopping may stop earlier)\n",
        "    per_device_train_batch_size=20, # Number of samples per batch for training\n",
        "    per_device_eval_batch_size=20, # Number of samples per batch for evaluation\n",
        "    logging_dir='./logs',          # Directory for training logs\n",
        "    logging_steps=1000,             # Log training info every 1000 steps\n",
        "    # no_cuda=True  # to force CPU usage if needed\n",
        "    eval_strategy=\"epoch\",              # Run evaluation after every epoch to check model performance\n",
        "    save_strategy=\"epoch\",                    # Save model checkpoint after each epoch\n",
        "    load_best_model_at_end=True,              # After training, load the checkpoint with best evaluation loss\n",
        "    metric_for_best_model=\"eval_loss\",        # Use evaluation loss to determine which checkpoint is best\n",
        "    weight_decay=0.01                         # Add weight decay (L2 regularization) to prevent overfitting\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2                 # Stop training if eval loss does not improve for 2 consecutive epochs\n",
        ")"
      ],
      "metadata": {
        "id": "m2j1G8c4Hlnj"
      },
      "id": "m2j1G8c4Hlnj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "e_V_MODSIgMj"
      },
      "id": "e_V_MODSIgMj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d283374-6c71-4bf6-809c-30286cc32e90",
      "metadata": {
        "id": "7d283374-6c71-4bf6-809c-30286cc32e90"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,          # function for evaluating metrics\n",
        "    callbacks=[early_stopping],               # Enable early stopping during training\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a342299-f44c-40e3-afd0-17f1fd0dff48",
      "metadata": {
        "id": "8a342299-f44c-40e3-afd0-17f1fd0dff48"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941d6959-03da-4b88-94fa-2f6c6241da86",
      "metadata": {
        "id": "941d6959-03da-4b88-94fa-2f6c6241da86"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=test_dataset,  # only evaluation dataset here\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate()  # evaluates the model on test_dataset\n",
        "print(eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is randomly guessing the correct answer, eval_loss: 1.1"
      ],
      "metadata": {
        "id": "TTSO_-bW_zbQ"
      },
      "id": "TTSO_-bW_zbQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myfinalproject)",
      "language": "python",
      "name": "myfinalproject"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}